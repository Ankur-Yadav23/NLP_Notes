{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\python311\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in d:\\python311\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in d:\\python311\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\python311\\lib\\site-packages (from nltk) (2023.5.5)\n",
      "Requirement already satisfied: tqdm in d:\\python311\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\vy788\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at d:\\python311\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install NLTK using pip\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vy788\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\vy788\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vy788\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vy788\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\vy788\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\vy788\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\vy788\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import NLTK\n",
    "import nltk\n",
    "\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "# The Punkt tokenizer is a pre-trained sentence tokenizer that can be used to split text into sentences.\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# These models are used for part-of-speech tagging, which assigns grammatical categories (such as noun, verb, adjective) to words in a sentence.\n",
    "nltk.download('maxent_ne_chunker')\n",
    "# NER is used to identify and categorize named entities (e.g., names of people, places, organizations) in text.\n",
    "nltk.download('words')\n",
    "# This resource is often used in various text analysis tasks to check the presence of words in a dictionary or vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"NLTK is a powerful tool for natural language processing. It can tokenize sentences and words. NLTK includes various NLP libraries for text analysis.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK is a powerful tool for natural language processing.', 'It can tokenize sentences and words.', 'NLTK includes various NLP libraries for text analysis.']\n",
      "Words: ['NLTK', 'is', 'a', 'powerful', 'tool', 'for', 'natural', 'language', 'processing', '.', 'It', 'can', 'tokenize', 'sentences', 'and', 'words', '.', 'NLTK', 'includes', 'various', 'NLP', 'libraries', 'for', 'text', 'analysis', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "# Each sentence in the text is separated and stored as elements in the sentences list.\n",
    "print(sentences)\n",
    "words = word_tokenize(text)\n",
    "# The words are separated based on spaces and punctuation, and they are stored in the words list.\n",
    "print(\"Words:\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('powerful', 'JJ'), ('tool', 'NN'), ('for', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.'), ('It', 'PRP'), ('can', 'MD'), ('tokenize', 'VB'), ('sentences', 'NNS'), ('and', 'CC'), ('words', 'NNS'), ('.', '.'), ('NLTK', 'NNP'), ('includes', 'VBZ'), ('various', 'JJ'), ('NLP', 'NNP'), ('libraries', 'NNS'), ('for', 'IN'), ('text', 'JJ'), ('analysis', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "pos_tags = pos_tag(words)\n",
    "print(\"POS Tags:\", pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noun (NN): Noun, Singular or Mass\n",
    "# Nouns (NNS): Noun, Plural\n",
    "# Proper Noun (NNP): Proper Noun, Singular\n",
    "# Proper Nouns (NNPS): Proper Noun, Plural\n",
    "# Adjective (JJ): Adjective\n",
    "# Adverb (RB): Adverb\n",
    "# Verb (VB): Verb, Base Form\n",
    "# Verbs (VBD): Verb, Past Tense\n",
    "# Verb (VBG): Verb, Gerund or Present Participle\n",
    "# Verb (VBN): Verb, Past Participle\n",
    "# Verb (VBP): Verb, Non-3rd Person Singular Present\n",
    "# Verb (VBZ): Verb, 3rd Person Singular Present\n",
    "# Pronoun (PRP): Personal Pronoun\n",
    "# Possessive Pronoun (PRP$): Possessive Pronoun\n",
    "# Determiner (DT): Determiner\n",
    "# Conjunction (CC): Coordinating Conjunction\n",
    "# Preposition (IN): Preposition or Subordinating Conjunction\n",
    "# Interjection (UH): Interjection\n",
    "# Modal (MD): Modal\n",
    "# Cardinal Number (CD): Cardinal Number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words: ['nltk', 'is', 'a', 'power', 'tool', 'for', 'natur', 'languag', 'process', '.', 'it', 'can', 'token', 'sentenc', 'and', 'word', '.', 'nltk', 'includ', 'variou', 'nlp', 'librari', 'for', 'text', 'analysi', '.']\n",
      "Lemmatized Words: ['NLTK', 'is', 'a', 'powerful', 'tool', 'for', 'natural', 'language', 'processing', '.', 'It', 'can', 'tokenize', 'sentence', 'and', 'word', '.', 'NLTK', 'includes', 'various', 'NLP', 'library', 'for', 'text', 'analysis', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(\"Stemmed Words:\", stemmed_words)\n",
    "print(\"Lemmatized Words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming:\n",
    "\n",
    "# Stemming is a simpler and more aggressive text normalization technique. It involves removing prefixes and suffixes from words to obtain a common root form. Stemmers do not consider the actual meaning of words, which can sometimes result in non-dictionary words. The most commonly used stemming algorithm is the Porter Stemmer.\n",
    "\n",
    "# For example:\n",
    "\n",
    "# \"Jumping\" -> \"Jump\"\n",
    "# \"Jumps\" -> \"Jump\"\n",
    "# \"Running\" -> \"Run\"\n",
    "# Stemming is useful in cases where you want to reduce words to their simplest form, but it may not always produce meaningful or grammatically correct words.\n",
    "\n",
    "# Lemmatization:\n",
    "\n",
    "# Lemmatization, on the other hand, is a more advanced text normalization technique that considers the context and meaning of words. It aims to reduce words to their base or dictionary form (known as the lemma) while taking into account grammatical rules and word relationships. Lemmatization is more accurate than stemming and generally results in valid words.\n",
    "\n",
    "# For example:\n",
    "\n",
    "# \"Running\" -> \"Run\"\n",
    "# \"Better\" -> \"Good\"\n",
    "# \"Cats\" -> \"Cat\"\n",
    "# Lemmatization is especially useful in applications where preserving the meaning of words is crucial, such as information retrieval, chatbots, and language translation. However, it is computationally more expensive than stemming due to the need to access a dictionary or a lexicon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Words: ['NLTK', 'powerful', 'tool', 'natural', 'language', 'processing', '.', 'tokenize', 'sentences', 'words', '.', 'NLTK', 'includes', 'various', 'NLP', 'libraries', 'text', 'analysis', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# the stopwords corpus contains a list of common stopwords in the English language.\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "print(\"Filtered Words:\", filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Distribution: <FreqDist with 22 samples and 26 outcomes>\n",
      "NLTK 2\n",
      "is 1\n",
      "a 1\n",
      "powerful 1\n",
      "tool 1\n",
      "for 2\n",
      "natural 1\n",
      "language 1\n",
      "processing 1\n",
      ". 3\n",
      "It 1\n",
      "can 1\n",
      "tokenize 1\n",
      "sentences 1\n",
      "and 1\n",
      "words 1\n",
      "includes 1\n",
      "various 1\n",
      "NLP 1\n",
      "libraries 1\n",
      "text 1\n",
      "analysis 1\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "freq_dist = FreqDist(words)\n",
    "print(\"Frequency Distribution:\", freq_dist)\n",
    "for i,j in freq_dist.items(): print(i,j)\n",
    "# The freq_dist object is a dictionary-like data structure where the keys are the unique words from the input list, and the values are the corresponding frequencies of each word in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concordance and Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 2 of 2 matches:\n",
      " NLTK is a powerful tool for natural langu\n",
      "t can tokenize sentences and words . NLTK includes various NLP libraries for t\n",
      "\n",
      "Concordance Result: None\n",
      "Similar Words: None\n"
     ]
    }
   ],
   "source": [
    "from nltk.text import Text\n",
    "\n",
    "text_object = Text(words)\n",
    "concordance_result = text_object.concordance(\"NLTK\")\n",
    "similar_words = text_object.similar(\"tool\")\n",
    "print(\"Concordance Result:\", concordance_result)\n",
    "print(\"Similar Words:\", similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Score: {'neg': 0.0, 'neu': 0.745, 'pos': 0.255, 'compound': 0.6705}\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sentiment_score = sia.polarity_scores(text)\n",
    "# This method calculates the sentiment scores for the text, which include values for positive, negative, neutral, and compound sentiments\n",
    "print(\"Sentiment Analysis Score:\", sentiment_score)\n",
    "# The compound score can range from -1 (very negative) to 1 (very positive), with 0 indicating a neutral sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Result: (S\n",
      "  (ORGANIZATION NLTK/NNP)\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  powerful/JJ\n",
      "  tool/NN\n",
      "  for/IN\n",
      "  natural/JJ\n",
      "  language/NN\n",
      "  processing/NN\n",
      "  ./.\n",
      "  It/PRP\n",
      "  can/MD\n",
      "  tokenize/VB\n",
      "  sentences/NNS\n",
      "  and/CC\n",
      "  words/NNS\n",
      "  ./.\n",
      "  (ORGANIZATION NLTK/NNP)\n",
      "  includes/VBZ\n",
      "  various/JJ\n",
      "  (ORGANIZATION NLP/NNP)\n",
      "  libraries/NNS\n",
      "  for/IN\n",
      "  text/JJ\n",
      "  analysis/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "# ne_chunk is used for Named Entity Recognition (NER), which is the process of identifying and classifying entities such as names of people, organizations, locations, dates, and more in a piece of text.\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags_for_ner = pos_tag(tokens)\n",
    "ner_result = ne_chunk(pos_tags_for_ner)\n",
    "print(\"NER Result:\", ner_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
